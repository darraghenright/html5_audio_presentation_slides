<!doctype html>
<html lang="en">

    <head>
        <meta charset="utf-8">

        <title>HTML5 Web Audio API</title>

        <meta name="description" content="A framework for easily creating beautiful presentations using HTML">
        <meta name="author" content="Hakim El Hattab">

        <meta name="apple-mobile-web-app-capable" content="yes" />
        <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

        <link rel="stylesheet" href="css/reveal.min.css">
        <link rel="stylesheet" href="css/theme/simple.css" id="theme">

        <!-- For syntax highlighting -->
        <link rel="stylesheet" href="lib/css/github.css">

        <style>

        /* override theme image styles */
        .reveal section img
        {
            border: none;
            box-shadow: none;
        }

        .reveal pre
        {
            box-shadow: none;
        }
        </style>

        <!-- If the query includes 'print-pdf', include the PDF print sheet -->
        <script>
            if( window.location.search.match( /print-pdf/gi ) ) {
                var link = document.createElement( 'link' );
                link.rel = 'stylesheet';
                link.type = 'text/css';
                link.href = 'css/print/pdf.css';
                document.getElementsByTagName( 'head' )[0].appendChild( link );
            }
        </script>

        <!--[if lt IE 9]>
        <script src="lib/js/html5shiv.js"></script>
        <![endif]-->
    </head>

    <body>

        <div class="reveal">

            <!-- Any section element inside of this container is displayed as a slide -->
            <div class="slides">

<!--  -->

<section>
    <h1>A quick peek at HTML5 Web Audio API</h1>
</section>

<section>
    <h2>00. Hello</h2>
</section>

<section>
    <img src="img/avatar.jpg" alt="">
    <p>twitter.com/darragh_enright</p>
</section>

<section>
    <h2>01. An incomplete history of computer music</h2>
</section>

<section>
    <h3>1950</h3>
    <p>CSIRAC is Australia's first digital computer. It is also the first computer ever to be used to create music.
    </p>
</section>

<section>
    <h3>1951</h3>
    <p>The Ferranti Mark 1 at the University of Manchester is used to create the  oldest recorded computer music, including <a href="https://www.youtube.com/watch?v=2i2ylXomcSo">"God save the King" and "Baa Baa Black Sheep"</a></p>
    <audio id="listen-baa" src="mp3/ferranti.mp3" controls></audio>
    <script>
    var baa = document.querySelector('#listen-baa');
    baa.addEventListener('canplaythrough', function() {
      this.currentTime = 32;
      //this.play();
    });
    </script>
</section>

<section>
    <h3>1957</h3>
    <p>Max Mathews writes MUSIC at Bell Labs. It's the first computer program for generating digital audio waveforms through direct synthesis.
    <p class="fragment">It is the first in a series of MUSIC-N software, which becomes inspiration for many subsequent computer music applications.</p>
</section>

<section>
    <h3>1961</h3>
    <p>The IBM 7090 is the first computer to "sing" a song called <a href="https://www.youtube.com/watch?v=41U78QP8nBk">"Daisy Bell"</a></p>
    <div><img src="img/7090.jpg"></div>
    <div><audio id="listen-daisy" src="mp3/daisy.mp3" controls></audio></div>
    <script>
    var daisy = document.querySelector('#listen-daisy');
    daisy.addEventListener('canplaythrough', function() {
      this.currentTime = 64;
      //this.play();
    });
    </script>
</section>

<section>
    <p>:o</p>
    <img src="img/hal.jpg">
</section>

<section>
    <h3>^197[0-9]$</h3>
    <p>Development of FM synthesis</p>
    <img src="img/fm.png" alt="">
    <p class="fragment">And... Probably lots of other really exciting stuff?</p>
</section>

<section>
    <img src="img/arp.jpg" alt="">
</section>

<section>
    <h2>1982</h2>
    <p>Commodore 64 is released. 8-bit computers are starting to become commonplace in the home. Games == music.</p>
</section>

<section>
    <h3>1983</h3>
    <p>MIDI (Music Instrument Digital Interface) Specification is published, defining a protocol to allow music instruments (and eventually computers) to talk to each other</p>
</section>

<section>
    <h3>1984</h3>
    <p>Barry Vercoe creates CSOUND</p>
    <pre><code>
          instr 2
a1        oscil     p4, p5, 1      ; p4=amp
          out       a1             ; p5=freq
          endin
    </code></pre>

<pre><code>
f1   0    4096 10 1      ; sine wave

;ins strt dur  amp(p4)   freq(p5)
i2   0    1    2000      880
i2   1.5  1    4000      440
i2   3    1    8000      220
i2   4.5  1    16000     110
i2   6    1    32000     55

e
</code></pre>
</section>

<section>

<h3>1985</h3>
<p>Atari ST is released</p>
</section>
<section>
<h3>1987</h3>
<p>Commodore Amiga 500 is released. The Amiga quickly becomes a popular platform for Music Tracker software</p>
<img src="img/tracker.png" alt="">
</section>
<section>
<h3>1989</h3>
<p>MIDI sequencer Cubase 1.0 is released on the Atari ST</p>
<img src="img/cubase.jpg">
</section>

<section>
    <h3>^199[0-9]$</h3>
    <p>By now there are tons of freeware/shareware software available on the web.</p>
</section>

<section>
    <h4>Stomper Ultra++</h4>
    <img src="img/stomper.gif" alt="">
</section>

<section>
    <h4>HammerHead Rhythm Station</h4>
    <img src="img/hammerhead.gif" alt="">
</section>

<section>
    <h4>Vaz Plus</h4>
    <img src="img/vaz.jpg" alt="">
</section>

<section>
    <h4>Rebirth RB-338</h4>
    <img src="img/rebirth.png" alt="">
</section>

<section>
    <p>etc.</p>
</section>

<section>
    <h2>Present Day</h2>
</section>

<section>
    <p>Computer based recording and synthesis is incredibly powerful.</p>
    <p>Run a real-time studio environment in a domestic machine: recording, synthesis, effects, mixing etc.</p>
    <img src="img/logic.jpg" alt="">
</section>

<section>
    <h2>But.. what does this have to do with HTML5?</h2>
</section>

<section>
    <p>Not a lot? Most computer music software runs natively, written for high performance, typically in languages like C/C++.</p>
    <p>But...</p>
</section>

<section>
    <h3>May 23rd, 2012</h3>
    <p>Google released a <a href="http://www.google.com/doodles/robert-moogs-78th-birthday">Doodle</a> celebrating the 78th (posthumous) birthday of Robert Moog</p>
    <img src="img/moogle.png" alt="">
    <p>A playable, recordable Minimoog (moogle?) in your browser!</p>
</section>

<section>
    <p>
        "Much like the musical machines Bob Moog created, this doodle was synthesized from a number of smaller components to form a unique instrument. <strong>When experienced with Google Chrome, sound is generated natively using the Web Audio API</strong> â€” a doodle first (for other browsers the Flash plugin is used)."
    </p>
    <h3>Wait... what?</h3>
</section>

<section>
    <p>HTML5 has a Web Audio API!?</p>
</section>

<section>
    "Any application that can be written in JavaScript, will eventually be written in JavaScript." - <a href="http://blog.codinghorror.com/the-principle-of-least-power/">Atwood's Law</a>
</section>

<section>
    <img src="img/html5.png" alt="">
    <h2>02. HTML5 Web Audio API</h2>
</section>

<section>
    <p>More than just the <code>&lt;audio&gt;</code> tag</p>
</section>

<section>
    <p>"HTML5 Audio is a subject of the HTML5 specification, investigating audio input, playback, synthesis, as well as speech to text in the browser." - <a href="http://en.wikipedia.org/wiki/HTML5_Audio">Wikipedia</a></p>
</section>

<section>
    <img src="img/caniuse.png" alt="">
</section>

<section>
    <h3>W3C Working Draft 10 October 2013</h3>
    <a href="http://www.w3.org/TR/webaudio/">http://www.w3.org/TR/webaudio/</a>
</section>

<section>
    <h3>Features?</h3>
    <a href="http://www.w3.org/TR/webaudio/#Features">W3C Working Draft 10 October 2013</a>
</section>

<section>
    <section>Modular routing for simple or complex mixing/effect architectures, including multiple sends and submixes.</section>

<section>Sample-accurate scheduled sound playback with low latency for musical applications requiring a very high degree of rhythmic precision such as drum machines and sequencers. This also includes the possibility of dynamic creation of effects.</section>

<section>Automation of audio parameters for envelopes, fade-ins / fade-outs, granular effects, filter sweeps, LFOs etc.</section>

<section>Flexible handling of channels in an audio stream, allowing them to be split and merged.</section>

<section>Processing of audio sources from an audio or video media element.</section>

<section>Processing live audio input using a MediaStream from getUserMedia().</section>

<section>Integration with WebRTC</section>

<section>Audio stream synthesis and processing directly in JavaScript.</section>

<section>Spatialized audio supporting a wide range of 3D games and immersive environments; e.g: panning, distance etc.</section>

<section>A convolution engine for a wide range of linear effects, especially very high-quality room effects; e.g: room, hall, backwards/extreme etc.</section>

<section>Dynamics compression for overall control and sweetening of the mix.</section>

<section>Efficient real-time time-domain and frequency analysis / music visualizer support.</section>

<section>Efficient biquad filters for lowpass, highpass, and other common filters.</section>

<section>A Waveshaping effect for distortion and other non-linear effects</section>

<section>Oscillators</section>
</section>

<section>
    <h3>Modular Routing</h3>
    <img src="img/modular.jpg" alt="">
</section>

<section>
    <p>"Modular routing allows arbitrary connections between different AudioNode objects. Each node can have inputs and/or outputs. A source node has no inputs and a single output. A destination node has one input and no outputs"</p>
    <img class="fragment" src="img/system.gif" alt="">
</section>

<section>
    <h2>03. Nodes and Example Usage</h2>
</section>

<section>
    <h3>AudioContext()</h3>
    <p>"Represents a set of <code>AudioNode</code> objects and their connections. It allows for arbitrary routing of signals to the <code>AudioDestinationNode</code> (what the user ultimately hears)."</p>
        <a href="http://www.w3.org/TR/webaudio/#AudioContext-section">#AudioContext-section</a>
</section>

<section>
    "In most use cases, only a single <code>AudioContext</code> is used per document."
</section>

<section>
<p>"Nodes are created from the context and are then connected together."</p>
    <pre><code>
// webkitAudioContext in Chrome
var context = new AudioContext();

// create oscillator (for waveform synthesis)
var oscillator = context.createOscillator();

// create gain (for volume control/mixing)
var gain = context.createGainNode();

// create filter (for sound shaping)
var filter = context.createBiquadFilter();

// create source (for sample playback)
var source = context.createBufferSource();

// etc.
    </code></pre>
</section>

<section>
    <h3>AudioBufferSourceNode()</h3>
    "Represents an audio source from an in-memory audio asset in an <code>AudioBuffer</code>. It is useful for playing short audio assets which require a high degree of scheduling flexibility."
    <img src="img/waveform.gif" alt="">
</section>

<section>
 <a href="http://www.w3.org/TR/webaudio/#AudioBufferSourceNode-section">#AudioBufferSourceNode-section</a>
<pre><code>interface AudioBufferSourceNode : AudioNode {

    attribute AudioBuffer? buffer;

    readonly attribute AudioParam playbackRate;

    attribute boolean loop;
    attribute double  loopStart;
    attribute double  loopEnd;

    void start(optional double when = 0,
               optional double offset = 0,
               optional double duration);

    void stop(optional double when = 0);

    attribute EventHandler onended;
};
        </code>
    </pre>
</section>

<section>
<h3>Play a .wav file</h3>
<pre><code>var file = 'http://localhost/break/melvin.wav',
    context,
    request;

// create an AudioContext object
// note webkit* prefix here
context = new webkitAudioContext(),

// load a file as an arraybuffer
request  = new XMLHttpRequest(),
request.open('GET', file, true);
request.responseType = 'arraybuffer';

// load handler
request.onload = function() {

  // decode loaded arraybuffer data
  context.decodeAudioData(request.response, function(buffer) {

    // create AudioBufferSourceNode object
    var source = context.createBufferSource();

    // connect to AudioDestinationNode (speakers)
    source.connect(context.destination);
    source.buffer = buffer;

    // playback options
    source.loop = true;
    source.loopStart = buffer.duration / 2;
    source.loopEnd = buffer.duration;
    source.playbackRate.value = 1;
    source.start(0);
  });
};

request.send();
</code></pre>
</section>

<section>
<h3>Play a .wav file</h3>
<a class="jsbin-embed" href="http://localhost:3000/raw/1/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h3>Trigger samples with keypress events</h3>
    <a href="http://localhost/trigger/trigger.html">View</a>
</section>

<section>
    <h3>OscillatorNode()</h3>
    <p>"Represents an audio source generating a periodic waveform. It can be set to a few commonly used waveforms. Additionally, it can be set to an arbitrary periodic waveform through the use of a <code>PeriodicWave</code> object."</p>
    <img src="img/waveform.png" alt="">
</section>

<section>
    <a href="http://www.w3.org/TR/webaudio/#OscillatorNode">#OscillatorNode</a>
<pre><code>interface OscillatorNode : AudioNode {

    attribute OscillatorType type;

    readonly attribute AudioParam frequency; // in Hertz
    readonly attribute AudioParam detune; // in Cents

    void start(double when);
    void stop(double when);
    void setPeriodicWave(PeriodicWave periodicWave);

    attribute EventHandler onended;

};</code></pre>
</section>

<section>
    <h3>Basic synthesis</h3>
    <p><code>OscillatorNode()</code> &#10141; <code>AudioDestinationNode()</code></p>
    <img src="img/_route_01.png" alt="">
</section>

<section>
    <h3>Basic synthesis</h3>
<pre><code>var context,
    oscillator;

// create an AudioContext object
context = new webkitAudioContext();

// create an OscillatorNode object
oscillator = context.createOscillator();

// wire up nodes
// OscillatorNode -> AudioDestinationNode
oscillator.connect(context.destination);

// now we can make some noise...

// sine, triangle, square, sawtooth, custom
oscillator.type = 'sine';
oscillator.frequency.value = 220;

// play note for 2 seconds
oscillator.start(0);
  oscillator.stop(2);

</code></pre>
</section>

<section>
<h3>Basic synthesis</h3>
<a class="jsbin-embed" href="http://localhost:3000/tex/4/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>
<section>
    <h3>PeriodicWave()</h3>
    <p>Customised waveforms</p>
<a class="jsbin-embed" href="http://localhost:3000/haf/2/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h3>GainNode()</h3>
    <p>"Changing the gain of an audio signal is a fundamental operation in audio applications. The GainNode is one of the building blocks for creating mixers."</p>
    <img src="img/gain.jpg" alt="">
</section>

<section>
    <a href="http://www.w3.org/TR/webaudio/#GainNode-section">#GainNode-section</a>
    <pre><code>
interface GainNode : AudioNode {
    attribute AudioParam gain;
};
    </code></pre>
</section>

<section>
    <h3>Adding volume control</h3>
    <p><code>OscillatorNode()</code> &#10141; <code>GainNode()</code> &#10141; <code>AudioDestinationNode()</code></p>
    <img src="img/_route_02.png" alt="">
</section>

<section>
    <h3>Adding volume control</h3>
    <a class="jsbin-embed" href="http://localhost:3000/kag/1/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h3>AudioParam()</h3>
    <p>"Controls an individual aspect of an <code>AudioNode</code>'s functioning, such as volume. The parameter can be set immediately to a particular value using the value attribute. Or, value changes can be scheduled to happen at very precise times for envelopes, volume fades, LFOs, filter sweeps, grain windows, etc."</p>
</section>

<section>
    <a href="http://www.w3.org/TR/webaudio/#AudioParam">#AudioParam</a>
    <pre><code>
interface AudioParam {

    attribute float value;
    readonly attribute float defaultValue;

    // Parameter automation.
    void setValueAtTime(float value, double startTime);
    void linearRampToValueAtTime(float value, double endTime);
    void exponentialRampToValueAtTime(float value, double endTime);

    // Exponentially approach the target value
    // with a rate having the given time constant.
    void setTargetAtTime(float target,
                         double startTime,
                         double timeConstant);

    // Sets an array of arbitrary parameter values
    // starting at time for the given duration.
    // The number of values will be scaled to
    // fit into the desired duration.
    void setValueCurveAtTime(Float32Array values,
                             double startTime,
                             double duration);

    // Cancels all scheduled parameter changes
    // with times greater than or equal to startTime.
    void cancelScheduledValues(double startTime);

};
    </code></pre>
</section>

<section>
    <h3>Amplitude Modulation (AM)</h3>
    <img src="img/am.png" alt="">
</section>

<section>
    <p>"<strong>Amplitude Modulation Synthesis</strong> is a type of sound synthesis where the gain of one signal is controlled, or modulated, by the gain of another signal. The signal whose gain is being modulated is called the <strong>carrier</strong>, and the signal responsible for the modulation is called the <strong>modulator</strong>."</p>
    <a href="http://en.flossmanuals.net/pure-data/audio-tutorials/amplitude-modulation/">flossmanuals.net/</a>
</section>

<section>
<img src="img/am_route.png" alt="">
<pre><code>
var context   = new AudioContext(),
    carrier   = context.createOscillator(),
    modulator = context.createOscillator(),
    gainNode  = context.createGainNode();

carrier.connect(gainNode);
gainNode.connect(context.destination);
modulator.connect(this.gainNode.gain);
    </code></pre>
</section>

<section>
    <p>AM Example #1</p>
    <a class="jsbin-embed" href="http://localhost:3000/jaj/1/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <p>AM Example #2</p>
    <a class="jsbin-embed" href="http://localhost:3000/mug/1/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h3>Frequency Modulation (FM)</h3>
    <img src="img/fm.png" alt="">
</section>

<section>
    <p>"<strong>Frequency Modulation Synthesis</strong> is used to make periodic changes to the frequency of an oscillator. In its simplest form, Frequency Modulation uses two oscillators. The first is the <strong>carrier</strong> oscillator, which is the one whose frequency will be changed over time. The second is the <strong>modulator</strong> oscillator, which will change the frequency of the carrier."</p>
    <a href="http://en.flossmanuals.net/pure-data/audio-tutorials/frequency-modulation/">flossmanuals.net</a>
</section>

<section>
<img src="img/fm_route.png" alt="">
<pre><code>
var context = new webkitAudioContext(),
    carrier = context.createOscillator(),
    modulator = context.createOscillator(),
    modulatorGain = context.createGainNode();

carrier.connect(context.destination);
modulator.connect(modulatorGain);
modulatorGain.connect(carrier.frequency);

</code></pre>
</section>

<section>
    <p>FM Example #1</p>
    <a class="jsbin-embed" href="http://localhost:3000/log/1/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <p>FM Example #2</p>
    <a class="jsbin-embed" href="http://localhost:3000/cab/2/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h3>BiquadFilterNode()</h3>
    <p>"BiquadFilterNode is an AudioNode processor implementing very common low-order filters. Low-order filters are the building blocks of basic tone controls (bass, mid, treble), graphic equalizers, and more advanced filters. Multiple BiquadFilterNode filters can be combined to form more complex filters."</p>
    <a href="http://www.w3.org/TR/webaudio/#BiquadFilterNode">#BiquadFilterNode</a>
</section>

<section>
    <pre><code>enum BiquadFilterType {
  "lowpass",
  "highpass",
  "bandpass",
  "lowshelf",
  "highshelf",
  "peaking",
  "notch",
  "allpass"
};
    </code></pre>
</section>

<section>
    <pre><code>
interface BiquadFilterNode : AudioNode {

    attribute BiquadFilterType type;
    readonly attribute AudioParam frequency; // in Hertz
    readonly attribute AudioParam detune; // in Cents
    readonly attribute AudioParam Q; // Quality factor
    readonly attribute AudioParam gain; // in Decibels

    void getFrequencyResponse(Float32Array frequencyHz,
                              Float32Array magResponse,
                              Float32Array phaseResponse);

};
    </code></pre>
</section>

<section>
    <p>Simple <code>BiquadFilterNode()</code> Example</p>
    <a class="jsbin-embed" href="http://localhost:3000/kud/2/embed?js,output">JS Bin</a><script src="http://localhost:3000/js/embed.js"></script>
</section>

<section>
    <h2>Thanks!</h2>
</section>


<!--  -->

            </div>

        </div>

        <script src="lib/js/head.min.js"></script>
        <script src="js/reveal.min.js"></script>

        <script>

            // Full list of configuration options available here:
            // https://github.com/hakimel/reveal.js#configuration
            Reveal.initialize({
                controls: true,
                progress: true,
                history: true,
                center: true,

                theme: Reveal.getQueryHash().theme, // available themes are in /css/theme
                transition: Reveal.getQueryHash().transition || 'linear', // default/cube/page/concave/zoom/linear/fade/none

                // Parallax scrolling
                // parallaxBackgroundImage: 'https://s3.amazonaws.com/hakim-static/reveal-js/reveal-parallax-1.jpg',
                // parallaxBackgroundSize: '2100px 900px',

                // Optional libraries used to extend on reveal.js
                dependencies: [
                    { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
                    { src: 'plugin/markdown/marked.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/markdown/markdown.js', condition: function() { return !!document.querySelector( '[data-markdown]' ); } },
                    { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
                    { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
                    { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
                ]
            });

        </script>

    </body>
</html>
